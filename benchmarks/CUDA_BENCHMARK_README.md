# PyTorch vs CUDA vs torch.compile Benchmarking

This benchmarking system compares three implementations of matrix multiplication:

1. **Original PyTorch**: The native PyTorch implementation from `sample_torch_script.py`
2. **torch.compile**: The same function optimized with PyTorch's built-in compiler
3. **CUDA Implementation**: The custom CUDA kernel generated by Gemini via the optimizer_cli

## Files

- `setup.py`: Configures the PyTorch CUDA extension
- `pytorch_cuda_benchmark.py`: Main benchmarking script
- `opt_runs_pytorch/inefficient_matmul_loop_cuda_implementation_1_cleaned.cu`: The optimized CUDA implementation

## Requirements

- PyTorch with CUDA support
- CUDA toolkit with nvcc compiler
- numpy

## Setup

1. Ensure the CUDA extension is built:
```bash
cd TorchFuser
python pytorch_cuda_benchmark.py --build
```

This will compile the CUDA extension as a PyTorch extension module.

## Running the Benchmark

```bash
cd TorchFuser
python pytorch_cuda_benchmark.py
```

You can specify the number of benchmark runs:
```bash
python pytorch_cuda_benchmark.py --runs 10
```

## How It Works

The benchmark system:
1. Loads the original PyTorch implementation from `pytorch_optimizer_cli/sample_torch_script.py`
2. Creates and runs a `torch.compile` version of the same function
3. Loads the compiled CUDA extension and runs the optimized CUDA implementation
4. Measures execution time for each implementation over multiple runs
5. Calculates and reports average execution times and speedups

## Understanding the CUDA Implementation

The CUDA implementation uses several optimizations:

1. **Shared Memory**: Uses shared memory to reduce global memory accesses
2. **Tiled Matrix Multiplication**: Divides matrices into tiles for better cache utilization
3. **Kernel Fusion**: Combines multiplication and addition operations when possible
4. **Thread Coalescing**: Organized memory access patterns for better performance

## Expected Performance

Typically, you should see the following performance ranking:
1. CUDA Implementation (fastest)
2. torch.compile version
3. Original PyTorch (slowest)

The actual speedup will depend on your hardware, particularly your GPU model and memory bandwidth. 